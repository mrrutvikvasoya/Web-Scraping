{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87e6d781",
   "metadata": {},
   "source": [
    "### **Overview of the code:**\n",
    "\n",
    "This module provides functions to scrape web pages for links, images, and text content.\n",
    "\n",
    "#### **Components and Functionalities**\n",
    "\n",
    "1. **Imports**:\n",
    "   - Imports necessary modules (`requests`, `BeautifulSoup`, `urllib.parse`, `re`).\n",
    "\n",
    "2. **Function: `get_all_links(url, domain)`**:\n",
    "   - Retrieves all links from a webpage that belong to a specified domain.\n",
    "   - **Args**:\n",
    "     - `url (str)`: The URL of the webpage.\n",
    "     - `domain (str)`: The domain to filter links.\n",
    "   - **Returns**:\n",
    "     - `set`: A set of links that belong to the specified domain.\n",
    "\n",
    "3. **Function: `scrape_page(url)`**:\n",
    "   - Scrapes images and text content from a webpage.\n",
    "   - **Args**:\n",
    "     - `url (str)`: The URL of the webpage.\n",
    "   - **Returns**:\n",
    "     - `tuple`: A tuple containing a list of image URLs and the text content.\n",
    "\n",
    "#### **Detailed Method Descriptions**\n",
    "\n",
    "- **`get_all_links(url, domain)`**:\n",
    "  - Makes an HTTP GET request to `url` using `requests.get`.\n",
    "  - Parses the HTML content using `BeautifulSoup`.\n",
    "  - Finds all `<a>` tags with `href` attributes and filters them by the specified `domain`.\n",
    "  - Constructs absolute URLs using `urljoin` and adds them to a set of links.\n",
    "\n",
    "- **`scrape_page(url)`**:\n",
    "  - Makes an HTTP GET request to `url` using `requests.get`.\n",
    "  - Parses the HTML content using `BeautifulSoup`.\n",
    "  - **Image Scraping**:\n",
    "    - Finds all `<img>` tags with `src` attributes and filters out data URLs.\n",
    "    - Constructs absolute image URLs using `urljoin`.\n",
    "  - **Text Scraping**:\n",
    "    - Extracts all text content from the webpage, excluding content from `<header>` and `<footer>` tags.\n",
    "    - Cleans and preprocesses the text using regular expressions (`re`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee01e9-c77b-4fb1-8bb7-7b156d301a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module for scraping web pages.\n",
    "\"\"\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import re\n",
    "\n",
    "def get_all_links(url, domain):\n",
    "    \"\"\"\n",
    "    Get all links from a webpage.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the webpage.\n",
    "        domain (str): The domain to filter links.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of links that belong to the specified domain.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        sourp = BeautifulSoup(response.content, 'html.parser')\n",
    "        links = set()\n",
    "\n",
    "        # Extract and filter links by domain\n",
    "        for tag in sourp.find_all('a', href=True):\n",
    "            href = tag['href']\n",
    "            full_url = urljoin(url, href)\n",
    "            if urlparse(full_url).netloc:\n",
    "                links.add(full_url)\n",
    "\n",
    "        return links\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return set()\n",
    "\n",
    "def scrape_page(url):\n",
    "    \"\"\"\n",
    "    Scrape images and text from a webpage.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the webpage.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing a list of image URLs and the text content.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        sourp = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Get all image URLs and filter out data URLs\n",
    "\n",
    "        # Get all text content\n",
    "\n",
    "        for tag in sourp(['header', 'footer']):\n",
    "            tag.decompose()\n",
    "\n",
    "        # Extract text from remaining content\n",
    "        texts = ' '.join(sourp.stripped_strings)\n",
    "\n",
    "        # Pre-process the text\n",
    "        # Remove extra whitespace and newlines\n",
    "        texts = re.sub(r'[@#\\$%\\^&\\*\\(\\)_\\+\\=\\{\\}\\[\\]\\|\\\\:;\\\"\\'<>,\\.\\?/~\\-\\⇒\\©\\®\\™\\§\\¶•…¡¿÷×°‰‡†¶©®€£]', '', texts)\n",
    "        \n",
    "        images = [urljoin(url, img['src']) for img in sourp.find_all('img', src=True) if not img['src'].startswith('data:')]\n",
    "\n",
    "\n",
    "        return images, texts\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return [], \"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
